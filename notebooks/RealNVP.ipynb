{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted norm convolutional 2D normalization \n",
    "class WNConv2d(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, kernel_size, **kwargs):\n",
    "        super(WeightNormConv2d, self).__init__()\n",
    "        self.conv = nn.utils.weight_norm(\n",
    "            nn.Conv2d(in_dim, out_dim, kernel_size, kwargs))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "# Resnet block using the weighted norm\n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, filters):\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            WeightNormConv2d(filters, filters, (1, 1), stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            WeightNormConv2d(filters, filters, (3, 3), stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            WeightNormConv2d(filters, filters, (1, 1), stride=1, padding=0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "# Activation normalization that adds scale and translation to each channel and using data dependent normalization\n",
    "class ActNorm(nn.Module):\n",
    "    def __init__(self, n_channels):\n",
    "        super(ActNorm, self).__init__()\n",
    "        self.log_scale = nn.Parameter(torch.zeros(1, n_channels, 1, 1), requires_grad = True) # scale factor (s) in paper\n",
    "        self.bias = nn.Parameter(torch.zeros(1, n_channels, 1, 1), requires_grad = True) # translation factor\n",
    "        self.channels = n_channels\n",
    "        self.initialized = False\n",
    "\n",
    "    def forward(self, x, reverse = False):\n",
    "        if reverse:\n",
    "            return (x - self.bias) * torch.exp(-self.log_scale), self.log_scale\n",
    "        if not self.initialized:\n",
    "            self.log_scale.data = torch.log(x.permute(1, 0, 2, 3).reshape(self.channels, -1), dim = 1).view(1, self.channels, 1, 1)\n",
    "            self.bias.data = torch.mean(x.permute(1, 0, 2, 3).reshape(self.channels, -1), dim = 1).view(1, self.channels, 1, 1)\n",
    "            self.initialized = True\n",
    "        return x * torch.exp(self.log_scale) + self.bias, self.log_scale\n",
    "\n",
    "class Resnet(nn.Module):\n",
    "    def __init__(self, in_channels = 3, out_channels = 6, filters = 128, blocks = 8):\n",
    "        super(Resnet, self).__init__()\n",
    "        layers = []\n",
    "        layers.extend([WNConv2d(in_channels, filters, (3, 3), stride = 1, padding = 1),\n",
    "            nn.ReLU()])\n",
    "        for _ in range(blocks):\n",
    "            layers.append(ResnetBlock(filters))\n",
    "        layers.extend([nn.ReLU(),\n",
    "            WNConv2d(filters, out_channels, (3, 3), stride = 1, padding = 1)])\n",
    "        self.resnet = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.resnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AffineCheckerboardTransform(nn.Module):\n",
    "    def __init__(self, type=1.0):\n",
    "        super(AffineCheckerboardTransform, self).__init__()\n",
    "        self.mask = self.build_mask(type=type)\n",
    "        self.scale = nn.Parameter(torch.zeros(1), requires_grad=True)\n",
    "        self.scale_shift = nn.Parameter(torch.zeros(1), requires_grad=True)\n",
    "        self.resnet = Resnet(in_channels = 3, out_channels = 6)\n",
    "\n",
    "    def build_mask(self, type=1.0):\n",
    "        # if type == 1.0, the top left corner will be 1.0 else on type == 0.0 it will be 0.0\n",
    "        mask = np.arange(32).reshape(-1, 1) + np.arange(32)\n",
    "        mask = np.mod(type + mask, 2)\n",
    "        mask = mask.reshape(-1, 1, 32, 32)\n",
    "        return torch.tensor(mask.astype('float32')).to(device)\n",
    "\n",
    "    def forward(self, x, reverse=False):\n",
    "        # returns transform(x), log_det\n",
    "        batch_size, n_channels, _, _ = x.shape\n",
    "        mask = self.mask.repeat(batch_size, 1, 1, 1)\n",
    "        x_ = x * mask\n",
    "        \n",
    "        # from pseudo-code provided\n",
    "        log_s, t = self.resnet(x_).split(n_channels, dim=1)\n",
    "        log_s = self.scale * torch.tanh(log_s) + self.scale_shift # both scale and scale_shift learnable params\n",
    "        t = t * (1.0 - mask) # for the other half of the x\n",
    "        log_s = log_s * (1.0 - mask) # for the other half of the x\n",
    "\n",
    "        if reverse:  # inverting the transformation\n",
    "            x = (x - t) * torch.exp(-log_s) # for inverse\n",
    "        else:\n",
    "            x = x * torch.exp(log_s) + t # for forward\n",
    "        return x, log_s\n",
    "\n",
    "class AffineChannelTransform(nn.Module):\n",
    "    def __init__(self, modify_top):\n",
    "        '''\n",
    "        modify_top : Signifies which half of x is activated\n",
    "        '''\n",
    "        super(AffineChannelTransform, self).__init__()\n",
    "        self.modify_top = modify_top\n",
    "        self.scale = nn.Parameter(torch.zeros(1), requires_grad=True)\n",
    "        self.scale_shift = nn.Parameter(torch.zeros(1), requires_grad=True)\n",
    "        self.resnet = Resnet(in_channels=6, out_channels=12)\n",
    "\n",
    "    def forward(self, x, reverse=False):\n",
    "        batch_size, n_channels, _, _ = x.shape\n",
    "        if self.modify_top:\n",
    "            on, off = x.split(n_channels // 2, dim=1)\n",
    "        else:\n",
    "            off, on = x.split(n_channels // 2, dim=1)\n",
    "        log_s, t = self.resnet(off).split(n_channels // 2, dim=1)\n",
    "        log_s = self.scale * torch.tanh(log_s) + self.scale_shift\n",
    "\n",
    "        if reverse:  # inverting the transformation\n",
    "            on = (on - t) * torch.exp(-log_s)\n",
    "        else:\n",
    "            on = on * torch.exp(log_s) + t\n",
    "\n",
    "        if self.modify_top:\n",
    "            return torch.cat([on, off], dim=1), torch.cat([log_s, torch.zeros_like(log_s)], dim=1)\n",
    "        else:\n",
    "            return torch.cat([off, on], dim=1), torch.cat([torch.zeros_like(log_s), log_s], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealNVP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RealNVP, self).__init__()\n",
    "        self.prior = torch.distributions.Normal(torch.tensor(0.).to(device), torch.tensor(1.).to(device)) # standard normal distribution\n",
    "        self.checker_transforms1 = nn.ModuleList([\n",
    "            AffineCheckerboardTransform(1.0),\n",
    "            ActNorm(3),\n",
    "            AffineCheckerboardTransform(0.0),\n",
    "            ActNorm(3),\n",
    "            AffineCheckerboardTransform(1.0),\n",
    "            ActNorm(3),\n",
    "            AffineCheckerboardTransform(0.0)\n",
    "        ])\n",
    "\n",
    "        self.channel_transforms = nn.ModuleList([\n",
    "            AffineChannelTransform(True),\n",
    "            ActNorm(12),\n",
    "            AffineChannelTransform(False),\n",
    "            ActNorm(12),\n",
    "            AffineChannelTransform(True),\n",
    "        ])\n",
    "\n",
    "        self.checker_transforms2 = nn.ModuleList([\n",
    "            AffineCheckerboardTransform(1.0),\n",
    "            ActNorm(3),\n",
    "            AffineCheckerboardTransform(0.0),\n",
    "            ActNorm(3),\n",
    "            AffineCheckerboardTransform(1.0)\n",
    "        ])\n",
    "\n",
    "    def squeeze(self, x):\n",
    "        # C x H x W -> 4C x H/2 x W/2\n",
    "        B, C, H, W = x.size()\n",
    "        x = x.reshape(B, C, H // 2, 2, W // 2, 2)\n",
    "        x = x.permute(0, 1, 3, 5, 2, 4)\n",
    "        x = x.reshape(B, C * 4, H // 2, W // 2)\n",
    "        return x\n",
    "\n",
    "    def unsqueeze(self, x):\n",
    "        #  4C x H/2 x W/2  ->  C x H x W\n",
    "        B, C, H, W = x.size()\n",
    "        x = x.reshape(B, C // 4, 2, 2, H, W)\n",
    "        x = x.permute(0, 1, 4, 2, 5, 3)\n",
    "        x = x.reshape(B, C // 4, H * 2, W * 2)\n",
    "        return x\n",
    "\n",
    "    def g(self, z):\n",
    "        # z -> x (inverse of f)\n",
    "        x = z\n",
    "        for op in reversed(self.checker_transforms2):\n",
    "            x, _ = op.forward(x, reverse=True)\n",
    "        x = self.squeeze(x)\n",
    "        for op in reversed(self.channel_transforms):\n",
    "            x, _ = op.forward(x, reverse=True)\n",
    "        x = self.unsqueeze(x)\n",
    "        for op in reversed(self.checker_transforms1):\n",
    "            x, _ = op.forward(x, reverse=True)\n",
    "        return x\n",
    "\n",
    "    def f(self, x):\n",
    "        # maps x -> z, and returns the log determinant (not reduced)\n",
    "        z, log_det = x, torch.zeros_like(x)\n",
    "        for op in self.checker_transforms1:\n",
    "            z, delta_log_det = op.forward(z)\n",
    "            log_det += delta_log_det\n",
    "        z, log_det = self.squeeze(z), self.squeeze(log_det)\n",
    "        for op in self.channel_transforms:\n",
    "            z, delta_log_det = op.forward(z)\n",
    "            log_det += delta_log_det\n",
    "        z, log_det = self.unsqueeze(z), self.unsqueeze(log_det)\n",
    "        for op in self.checker_transforms2:\n",
    "            z, delta_log_det = op.forward(z)\n",
    "            log_det += delta_log_det\n",
    "        return z, log_det\n",
    "\n",
    "    def log_prob(self, x):\n",
    "        z, log_det = self.f(x)\n",
    "        # equation 3 RealNVP paper\n",
    "        return torch.sum(log_det, dim = [1, 2, 3]) + torch.sum(self.prior.log_prob(z), dim = [1, 2, 3]) \n",
    "\n",
    "    def sample(self, num_samples):\n",
    "        z = self.prior.sample([num_samples, 3, 32, 32])\n",
    "        return self.g(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
